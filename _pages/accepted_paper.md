---
layout: page
permalink: /accepted_paper/
title: Accepted Paper
description: Our Frontier4LCD workshop has received 134 outstanding submissions, and we are thrilled to announce that 100 high-quality papers have been accepted for presentation. Below is the list of accepted papers.
nav: true
nav_order: 2
---

<br>

<div>
<table class="table" id="standings" style="border-collapse:collapse">
<tr class="header" style="background-color:rgb(215, 215, 215); border-top: 1pt solid white; border-bottom: 1pt solid black;">
        <th style="border-top-left-radius: 10px; width: 15%">Number</th>
        <!-- <th>Virtual link</th> -->
        <th style="width: 85%">Title</th>
        <!-- <th style="width: 15% border-top-right-radius: 10px;">Authors</th> -->
        <!-- <th style="width: 25% border-top-right-radius: 10px;">Speakers (Affiliations)</th> -->
      </tr>
      <tr>
  <!-- <tr class="header" style="cursor: pointer">
    <td>Number</td>
    <td><b>On Optimal Control and Machine Learning</b><br> -->

<!-- <tr class="header"><td>1</td><td>AbODE: Ab initio antibody design using conjoined ODEs</td><td></td></tr> -->
<tr class="header"><td>1</td><td>AbODE: Ab initio antibody design using conjoined ODEs</td></tr>
<tr class="header"><td>2</td><td>Distributional Distance Classifiers for Goal-Conditioned Reinforcement Learning</td></tr>
<tr class="header"><td>3</td><td>LEAD: Min-Max Optimization from a Physical Perspective</td></tr>
<tr class="header"><td>4</td><td>Balancing exploration and exploitation in Partially Observed Linear Contextual Bandits via Thompson Sampling</td></tr>
<tr class="header"><td>5</td><td>Stochastic Linear Bandits with Unknown Safety Constraints and Local Feedback</td></tr>
<tr class="header"><td>6</td><td>Visual Dexterity: In-hand Dexterous Manipulation from Depth</td></tr>
<tr class="header"><td>7</td><td>Regret Bounds for Risk-sensitive Reinforcement Learning with Lipschitz Dynamic Risk Measures</td></tr>
<tr class="header"><td>8</td><td>On learning history-based policies for controlling Markov decision processes</td></tr>
<tr class="header"><td>9</td><td>Improved sampling via learned diffusions</td></tr>
<tr class="header"><td>10</td><td>Fast Approximation of the Generalized Sliced-Wasserstein Distance</td></tr>
<tr class="header"><td>11</td><td>Synthetic Experience Replay</td></tr>
<tr class="header"><td>12</td><td>A neural RDE approach for continuous-time non-Markovian stochastic control problems</td></tr>
<tr class="header"><td>13</td><td>Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning</td></tr>
<tr class="header"><td>14</td><td>Toward Understanding Latent Model Learning in MuZero: A Case Study in Linear Quadratic Gaussian Control</td></tr>
<tr class="header"><td>15</td><td>Gradient-free training of neural ODEs for system identification and control using ensemble Kalman inversion</td></tr>
<tr class="header"><td>16</td><td>Preventing Reward Hacking with Occupancy Measure Regularization</td></tr>
<tr class="header"><td>17</td><td>Exponential weight averaging as damped harmonic motion</td></tr>
<tr class="header"><td>18</td><td>Bridging RL Theory and Practice with the Effective Horizon</td></tr>
<tr class="header"><td>19</td><td>Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning</td></tr>
<tr class="header"><td>20</td><td>Neural Optimal Transport with Lagrangian Costs</td></tr>
<tr class="header"><td>21</td><td>Taylor TD-learning</td></tr>
<tr class="header"><td>22</td><td>Coupled Gradient Flows for Strategic Non-Local Distribution Shift</td></tr>
<tr class="header"><td>23</td><td>Kernel Mirror Prox and RKHS Gradient Flow for Mixed Functional Nash Equilibrium</td></tr>
<tr class="header"><td>24</td><td>What is the Solution for State-Adversarial Multi-Agent Reinforcement Learning?</td></tr>
<tr class="header"><td>25</td><td>On the Generalization Capacities of Neural Controlled Differential Equations</td></tr>
<tr class="header"><td>26</td><td>A Best Arm Identification Approach for Stochastic Rising Bandits</td></tr>
<tr class="header"><td>27</td><td>Maximum State Entropy Exploration using Predecessor and Successor Representations</td></tr>
<tr class="header"><td>28</td><td>Guide Your Agent with Adaptive Multimodal Rewards</td></tr>
<tr class="header"><td>29</td><td>Breaking the Curse of Multiagents in a Large State Space: RL  in Markov Games with Independent  Linear Function Approximation</td></tr>
<tr class="header"><td>30</td><td>Unbalanced Optimal Transport meets Sliced-Wasserstein</td></tr>
<tr class="header"><td>31</td><td>Randomly Coupled Oscillators for Time Series Processing</td></tr>
<tr class="header"><td>32</td><td>Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware</td></tr>
<tr class="header"><td>33</td><td>Boosting Off-policy RL with Policy Representation and Policy-extended Value Function Approximator</td></tr>
<tr class="header"><td>34</td><td>Statistics estimation in neural network training: a recursive identification approach</td></tr>
<tr class="header"><td>35</td><td>Embedding Surfaces by Optimizing Neural Networks with Prescribed Riemannian Metric and Beyond</td></tr>
<tr class="header"><td>36</td><td>Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding</td></tr>
<tr class="header"><td>37</td><td>A Flexible Diffusion Model</td></tr>
<tr class="header"><td>38</td><td>Simulation-Free Schrödinger Bridges via Score and Flow Matching</td></tr>
<tr class="header"><td>39</td><td>On the Imitation of Non-Markovian  Demonstrations: From Low-Level Stability to High-Level Planning</td></tr>
<tr class="header"><td>40</td><td>Fixed-Budget Hypothesis Best Arm Identification: On the Information Loss in Experimental Design</td></tr>
<tr class="header"><td>41</td><td>Variational Principle and Variational Integrators for Neural Symplectic Forms</td></tr>
<tr class="header"><td>42</td><td>A Policy-Decoupled Method for High-Quality Data Augmentation in Offline Reinforcement Learning</td></tr>
<tr class="header"><td>43</td><td>When is Agnostic Reinforcement Learning Statistically Tractable?</td></tr>
<tr class="header"><td>44</td><td>Algorithms for Optimal Adaptation ofDiffusion Models to Reward Functions</td></tr>
<tr class="header"><td>45</td><td>On Convergence of Approximate Schr\"{o}dinger Bridge with Bounded Cost</td></tr>
<tr class="header"><td>46</td><td>In-Context Decision-Making from Supervised Pretraining</td></tr>
<tr class="header"><td>47</td><td>Unbalanced Diffusion Schrödinger Bridge</td></tr>
<tr class="header"><td>48</td><td>Learning from Sparse Offline Datasets via Conservative Density Estimation</td></tr>
<tr class="header"><td>49</td><td>Parameterized projected Bellman operator</td></tr>
<tr class="header"><td>50</td><td>Randomized methods for computing optimal transport without regularization and their convergence analysis</td></tr>
<tr class="header"><td>51</td><td>Bridging Physics-Informed Neural Networks with Reinforcement Learning: Hamilton-Jacobi-Bellman Proximal Policy Optimization (HJBPPO)</td></tr>
<tr class="header"><td>52</td><td>On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions</td></tr>
<tr class="header"><td>53</td><td>Dynamic Feature-based Newsvendor</td></tr>
<tr class="header"><td>54</td><td>Game Theoretic Neural ODE Optimizer</td></tr>
<tr class="header"><td>55</td><td>Diffusion Model-Augmented Behavioral Cloning</td></tr>
<tr class="header"><td>56</td><td>Vector Quantile Regression on Manifolds</td></tr>
<tr class="header"><td>57</td><td>PAC-Bayesian Bounds for Learning LTI-ss systems with Input from Empirical Loss</td></tr>
<tr class="header"><td>58</td><td>Learning to Optimize with Recurrent Hierarchical Transformers</td></tr>
<tr class="header"><td>59</td><td>Sample Complexity of Hierarchical Decompositions in Markov Decision Processes</td></tr>
<tr class="header"><td>60</td><td>Fairness In a Non-Stationary Environment From an Optimal Control Perspective</td></tr>
<tr class="header"><td>61</td><td>Modular Hierarchical Reinforcement Learning for Robotics: Improving Scalability and Generalizability</td></tr>
<tr class="header"><td>62</td><td>Taylorformer: Probabalistic Modelling for Random Processes including Time Series</td></tr>
<tr class="header"><td>63</td><td>Stability of Multi-Agent Learning: Convergence in Network Games with Many Players</td></tr>
<tr class="header"><td>64</td><td>Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport</td></tr>
<tr class="header"><td>65</td><td>Deep Equilibrium Based Neural Operators for Steady-State PDEs</td></tr>
<tr class="header"><td>66</td><td>Informed POMDP: Leveraging Additional Information in Model-Based RL</td></tr>
<tr class="header"><td>67</td><td>IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive Control</td></tr>
<tr class="header"><td>68</td><td>Modeling Accurate Long Rollouts with Temporal Neural PDE Solvers</td></tr>
<tr class="header"><td>69</td><td>Sub-linear Regret in Adaptive Model Predictive Control</td></tr>
<tr class="header"><td>70</td><td>Analyzing the Sample Complexity of Model-Free Opponent Shaping</td></tr>
<tr class="header"><td>71</td><td>Continuous Vector Quantile Regression</td></tr>
<tr class="header"><td>72</td><td>Trajectory Generation, Control, and Safety with Denoising Diffusion Probabilistic Models</td></tr>
<tr class="header"><td>73</td><td>Structured State Space Models for In-Context Reinforcement Learning</td></tr>
<tr class="header"><td>74</td><td>Latent Space Editing in Transformer-Based Flow Matching</td></tr>
<tr class="header"><td>75</td><td>Transport, VI, and Diffusions</td></tr>
<tr class="header"><td>76</td><td>Fit Like You Sample: Sample-Efficient Generalized Score Matching from Fast Mixing Markov Chains</td></tr>
<tr class="header"><td>77</td><td>Policy Gradient Algorithms Implicitly Optimize by Continuation</td></tr>
<tr class="header"><td>78</td><td>Improving Offline-to-Online Reinforcement Learning with Q-Ensembles</td></tr>
<tr class="header"><td>79</td><td>Offline Goal-Conditioned RL with Latent States as Actions</td></tr>
<tr class="header"><td>80</td><td>On the effectiveness of neural priors in modeling dynamical systems</td></tr>
<tr class="header"><td>81</td><td>Action and Trajectory Planning for Urban Autonomous Driving with Hierarchical Reinforcement Learning</td></tr>
<tr class="header"><td>82</td><td>Physics-informed Localized Learning for Advection-Diffusion-Reaction Systems</td></tr>
<tr class="header"><td>83</td><td>Factor Learning Portfolio Optimization Informed by Continuous-Time Finance Models</td></tr>
<tr class="header"><td>84</td><td>Equivalence Class Learning for GENERIC Systems</td></tr>
<tr class="header"><td>85</td><td>Model-based Policy Optimization under Approximate Bayesian Inference</td></tr>
<tr class="header"><td>86</td><td>Nonlinear Wasserstein Distributionally Robust Optimal Control</td></tr>
<tr class="header"><td>87</td><td>Online Control with Adversarial Disturbance for Continuous-time Linear Systems</td></tr>
<tr class="header"><td>88</td><td>Leveraging Factored Action Spaces for Off-Policy Evaluation</td></tr>
<tr class="header"><td>89</td><td>Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations</td></tr>
<tr class="header"><td>90</td><td>Limited Information Opponent Modeling</td></tr>
<tr class="header"><td>91</td><td>Tendiffpure: Tensorizing Diffusion Models for Purification</td></tr>
<tr class="header"><td>92</td><td>Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL</td></tr>
<tr class="header"><td>93</td><td>Optimization or Architecture: What Matters in Non-Linear Filtering?</td></tr>
<tr class="header"><td>94</td><td>Variational quantum dynamics of two-dimensional rotor models</td></tr>
<tr class="header"><td>95</td><td>Parallel Sampling of Diffusion Models</td></tr>
<tr class="header"><td>96</td><td>Actor-Critic Methods using Physics-Informed Neural Networks: Control of a 1D PDE Model for Fluid-Cooled Battery Packs</td></tr>
<tr class="header"><td>97</td><td>Undo Maps: A Tool for Adapting Policies to Perceptual Distortions</td></tr>
<tr class="header"><td>98</td><td>Learning with Learning Awareness using Meta-Values</td></tr>
<tr class="header"><td>99</td><td>Aligned Diffusion Schrödinger Bridges</td></tr>
<tr class="header"><td>100</td><td>On First-Order Meta-Reinforcement Learning with Moreau Envelopes</td></tr>


