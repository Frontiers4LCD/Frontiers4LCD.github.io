<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Accepted Paper | New Frontiers in Learning, Control, and Dynamical Systems</title> <meta name="author" content="New Frontiers in Learning, Control, and Dynamical Systems"/> <meta name="description" content="Our Frontier4LCD workshop has received 134 outstanding submissions, and we are thrilled to announce that 100 high-quality papers have been accepted for presentation. Below is the list of accepted papers."/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img//assets/img/cell"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://frontiers4lcd.github.io/accepted_paper/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">New Frontiers in <span class="font-weight-bold">Learning, Control, and Dynamical Systems</span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/accepted_paper/">Accepted Paper<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/schedule/">Schedule</a> </li> <li class="nav-item "> <a class="nav-link" href="/submissions/">Call for Papers</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Accepted Paper</h1> <p class="post-description">Our Frontier4LCD workshop has received 134 outstanding submissions, and we are thrilled to announce that 100 high-quality papers have been accepted for presentation. Below is the list of accepted papers.</p> </header> <article> <p><br></p> <div> <table class="table" id="standings" style="border-collapse:collapse"> <tr class="header" style="background-color:rgb(215, 215, 215); border-top: 1pt solid white; border-bottom: 1pt solid black;"> <th style="border-top-left-radius: 10px; width: 15%">Number</th> <th style="width: 85%">Title</th> </tr> <tr> </tr> <tr class="header"> <td>1</td> <td>AbODE: Ab initio antibody design using conjoined ODEs</td> </tr> <tr class="header"> <td>2</td> <td>Distributional Distance Classifiers for Goal-Conditioned Reinforcement Learning</td> </tr> <tr class="header"> <td>3</td> <td>LEAD: Min-Max Optimization from a Physical Perspective</td> </tr> <tr class="header"> <td>4</td> <td>Balancing exploration and exploitation in Partially Observed Linear Contextual Bandits via Thompson Sampling</td> </tr> <tr class="header"> <td>5</td> <td>Stochastic Linear Bandits with Unknown Safety Constraints and Local Feedback</td> </tr> <tr class="header"> <td>6</td> <td>Visual Dexterity: In-hand Dexterous Manipulation from Depth</td> </tr> <tr class="header"> <td>7</td> <td>Regret Bounds for Risk-sensitive Reinforcement Learning with Lipschitz Dynamic Risk Measures</td> </tr> <tr class="header"> <td>8</td> <td>On learning history-based policies for controlling Markov decision processes</td> </tr> <tr class="header"> <td>9</td> <td>Improved sampling via learned diffusions</td> </tr> <tr class="header"> <td>10</td> <td>Fast Approximation of the Generalized Sliced-Wasserstein Distance</td> </tr> <tr class="header"> <td>11</td> <td>Synthetic Experience Replay</td> </tr> <tr class="header"> <td>12</td> <td>A neural RDE approach for continuous-time non-Markovian stochastic control problems</td> </tr> <tr class="header"> <td>13</td> <td>Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning</td> </tr> <tr class="header"> <td>14</td> <td>Toward Understanding Latent Model Learning in MuZero: A Case Study in Linear Quadratic Gaussian Control</td> </tr> <tr class="header"> <td>15</td> <td>Gradient-free training of neural ODEs for system identification and control using ensemble Kalman inversion</td> </tr> <tr class="header"> <td>16</td> <td>Preventing Reward Hacking with Occupancy Measure Regularization</td> </tr> <tr class="header"> <td>17</td> <td>Exponential weight averaging as damped harmonic motion</td> </tr> <tr class="header"> <td>18</td> <td>Bridging RL Theory and Practice with the Effective Horizon</td> </tr> <tr class="header"> <td>19</td> <td>Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning</td> </tr> <tr class="header"> <td>20</td> <td>Neural Optimal Transport with Lagrangian Costs</td> </tr> <tr class="header"> <td>21</td> <td>Taylor TD-learning</td> </tr> <tr class="header"> <td>22</td> <td>Coupled Gradient Flows for Strategic Non-Local Distribution Shift</td> </tr> <tr class="header"> <td>23</td> <td>Kernel Mirror Prox and RKHS Gradient Flow for Mixed Functional Nash Equilibrium</td> </tr> <tr class="header"> <td>24</td> <td>What is the Solution for State-Adversarial Multi-Agent Reinforcement Learning?</td> </tr> <tr class="header"> <td>25</td> <td>On the Generalization Capacities of Neural Controlled Differential Equations</td> </tr> <tr class="header"> <td>26</td> <td>A Best Arm Identification Approach for Stochastic Rising Bandits</td> </tr> <tr class="header"> <td>27</td> <td>Maximum State Entropy Exploration using Predecessor and Successor Representations</td> </tr> <tr class="header"> <td>28</td> <td>Guide Your Agent with Adaptive Multimodal Rewards</td> </tr> <tr class="header"> <td>29</td> <td>Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation</td> </tr> <tr class="header"> <td>30</td> <td>Unbalanced Optimal Transport meets Sliced-Wasserstein</td> </tr> <tr class="header"> <td>31</td> <td>Randomly Coupled Oscillators for Time Series Processing</td> </tr> <tr class="header"> <td>32</td> <td>Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware</td> </tr> <tr class="header"> <td>33</td> <td>Boosting Off-policy RL with Policy Representation and Policy-extended Value Function Approximator</td> </tr> <tr class="header"> <td>34</td> <td>Statistics estimation in neural network training: a recursive identification approach</td> </tr> <tr class="header"> <td>35</td> <td>Embedding Surfaces by Optimizing Neural Networks with Prescribed Riemannian Metric and Beyond</td> </tr> <tr class="header"> <td>36</td> <td>Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding</td> </tr> <tr class="header"> <td>37</td> <td>A Flexible Diffusion Model</td> </tr> <tr class="header"> <td>38</td> <td>Simulation-Free Schrödinger Bridges via Score and Flow Matching</td> </tr> <tr class="header"> <td>39</td> <td>On the Imitation of Non-Markovian Demonstrations: From Low-Level Stability to High-Level Planning</td> </tr> <tr class="header"> <td>40</td> <td>Fixed-Budget Hypothesis Best Arm Identification: On the Information Loss in Experimental Design</td> </tr> <tr class="header"> <td>41</td> <td>Variational Principle and Variational Integrators for Neural Symplectic Forms</td> </tr> <tr class="header"> <td>42</td> <td>A Policy-Decoupled Method for High-Quality Data Augmentation in Offline Reinforcement Learning</td> </tr> <tr class="header"> <td>43</td> <td>When is Agnostic Reinforcement Learning Statistically Tractable?</td> </tr> <tr class="header"> <td>44</td> <td>Algorithms for Optimal Adaptation ofDiffusion Models to Reward Functions</td> </tr> <tr class="header"> <td>45</td> <td>On Convergence of Approximate Schr\"{o}dinger Bridge with Bounded Cost</td> </tr> <tr class="header"> <td>46</td> <td>In-Context Decision-Making from Supervised Pretraining</td> </tr> <tr class="header"> <td>47</td> <td>Unbalanced Diffusion Schrödinger Bridge</td> </tr> <tr class="header"> <td>48</td> <td>Learning from Sparse Offline Datasets via Conservative Density Estimation</td> </tr> <tr class="header"> <td>49</td> <td>Parameterized projected Bellman operator</td> </tr> <tr class="header"> <td>50</td> <td>Randomized methods for computing optimal transport without regularization and their convergence analysis</td> </tr> <tr class="header"> <td>51</td> <td>Bridging Physics-Informed Neural Networks with Reinforcement Learning: Hamilton-Jacobi-Bellman Proximal Policy Optimization (HJBPPO)</td> </tr> <tr class="header"> <td>52</td> <td>On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions</td> </tr> <tr class="header"> <td>53</td> <td>Dynamic Feature-based Newsvendor</td> </tr> <tr class="header"> <td>54</td> <td>Game Theoretic Neural ODE Optimizer</td> </tr> <tr class="header"> <td>55</td> <td>Diffusion Model-Augmented Behavioral Cloning</td> </tr> <tr class="header"> <td>56</td> <td>Vector Quantile Regression on Manifolds</td> </tr> <tr class="header"> <td>57</td> <td>PAC-Bayesian Bounds for Learning LTI-ss systems with Input from Empirical Loss</td> </tr> <tr class="header"> <td>58</td> <td>Learning to Optimize with Recurrent Hierarchical Transformers</td> </tr> <tr class="header"> <td>59</td> <td>Sample Complexity of Hierarchical Decompositions in Markov Decision Processes</td> </tr> <tr class="header"> <td>60</td> <td>Fairness In a Non-Stationary Environment From an Optimal Control Perspective</td> </tr> <tr class="header"> <td>61</td> <td>Modular Hierarchical Reinforcement Learning for Robotics: Improving Scalability and Generalizability</td> </tr> <tr class="header"> <td>62</td> <td>Taylorformer: Probabalistic Modelling for Random Processes including Time Series</td> </tr> <tr class="header"> <td>63</td> <td>Stability of Multi-Agent Learning: Convergence in Network Games with Many Players</td> </tr> <tr class="header"> <td>64</td> <td>Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport</td> </tr> <tr class="header"> <td>65</td> <td>Deep Equilibrium Based Neural Operators for Steady-State PDEs</td> </tr> <tr class="header"> <td>66</td> <td>Informed POMDP: Leveraging Additional Information in Model-Based RL</td> </tr> <tr class="header"> <td>67</td> <td>IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive Control</td> </tr> <tr class="header"> <td>68</td> <td>Modeling Accurate Long Rollouts with Temporal Neural PDE Solvers</td> </tr> <tr class="header"> <td>69</td> <td>Sub-linear Regret in Adaptive Model Predictive Control</td> </tr> <tr class="header"> <td>70</td> <td>Analyzing the Sample Complexity of Model-Free Opponent Shaping</td> </tr> <tr class="header"> <td>71</td> <td>Continuous Vector Quantile Regression</td> </tr> <tr class="header"> <td>72</td> <td>Trajectory Generation, Control, and Safety with Denoising Diffusion Probabilistic Models</td> </tr> <tr class="header"> <td>73</td> <td>Structured State Space Models for In-Context Reinforcement Learning</td> </tr> <tr class="header"> <td>74</td> <td>Latent Space Editing in Transformer-Based Flow Matching</td> </tr> <tr class="header"> <td>75</td> <td>Transport, VI, and Diffusions</td> </tr> <tr class="header"> <td>76</td> <td>Fit Like You Sample: Sample-Efficient Generalized Score Matching from Fast Mixing Markov Chains</td> </tr> <tr class="header"> <td>77</td> <td>Policy Gradient Algorithms Implicitly Optimize by Continuation</td> </tr> <tr class="header"> <td>78</td> <td>Improving Offline-to-Online Reinforcement Learning with Q-Ensembles</td> </tr> <tr class="header"> <td>79</td> <td>Offline Goal-Conditioned RL with Latent States as Actions</td> </tr> <tr class="header"> <td>80</td> <td>On the effectiveness of neural priors in modeling dynamical systems</td> </tr> <tr class="header"> <td>81</td> <td>Action and Trajectory Planning for Urban Autonomous Driving with Hierarchical Reinforcement Learning</td> </tr> <tr class="header"> <td>82</td> <td>Physics-informed Localized Learning for Advection-Diffusion-Reaction Systems</td> </tr> <tr class="header"> <td>83</td> <td>Factor Learning Portfolio Optimization Informed by Continuous-Time Finance Models</td> </tr> <tr class="header"> <td>84</td> <td>Equivalence Class Learning for GENERIC Systems</td> </tr> <tr class="header"> <td>85</td> <td>Model-based Policy Optimization under Approximate Bayesian Inference</td> </tr> <tr class="header"> <td>86</td> <td>Nonlinear Wasserstein Distributionally Robust Optimal Control</td> </tr> <tr class="header"> <td>87</td> <td>Online Control with Adversarial Disturbance for Continuous-time Linear Systems</td> </tr> <tr class="header"> <td>88</td> <td>Leveraging Factored Action Spaces for Off-Policy Evaluation</td> </tr> <tr class="header"> <td>89</td> <td>Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations</td> </tr> <tr class="header"> <td>90</td> <td>Limited Information Opponent Modeling</td> </tr> <tr class="header"> <td>91</td> <td>Tendiffpure: Tensorizing Diffusion Models for Purification</td> </tr> <tr class="header"> <td>92</td> <td>Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL</td> </tr> <tr class="header"> <td>93</td> <td>Optimization or Architecture: What Matters in Non-Linear Filtering?</td> </tr> <tr class="header"> <td>94</td> <td>Variational quantum dynamics of two-dimensional rotor models</td> </tr> <tr class="header"> <td>95</td> <td>Parallel Sampling of Diffusion Models</td> </tr> <tr class="header"> <td>96</td> <td>Actor-Critic Methods using Physics-Informed Neural Networks: Control of a 1D PDE Model for Fluid-Cooled Battery Packs</td> </tr> <tr class="header"> <td>97</td> <td>Undo Maps: A Tool for Adapting Policies to Perceptual Distortions</td> </tr> <tr class="header"> <td>98</td> <td>Learning with Learning Awareness using Meta-Values</td> </tr> <tr class="header"> <td>99</td> <td>Aligned Diffusion Schrödinger Bridges</td> </tr> <tr class="header"> <td>100</td> <td>On First-Order Meta-Reinforcement Learning with Moreau Envelopes</td> </tr> </table> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2023 New Frontiers in Learning, Control, and Dynamical Systems. Last updated: July 27, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">$(document).ready(function(){$("tr:not(.header)").hide(),$("tr.header").click(function(){$(this).find("span").text(function(t,n){return"-"==n?"+":"-"}),$(this).nextUntil("tr.header").slideToggle(1,function(){})})});</script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>